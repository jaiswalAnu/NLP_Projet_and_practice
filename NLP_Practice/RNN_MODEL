{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeHcoKss85pST//cU6aLAL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","!pip install transformers\n","from transformers import BertTokenizer, BertModel\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Set the path for the output file\n","path = '/content/output_file.csv'\n","\n","# Load the dataset from the CSV file\n","data = pd.read_csv(path)\n","\n","# Split the dataset into training and testing sets\n","X = data['text']\n","y = data['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize and encode the training data\n","X_train_encodings = tokenizer.batch_encode_plus(\n","    X_train.tolist(),\n","    truncation=True,\n","    padding=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","# Tokenize and encode the testing data\n","X_test_encodings = tokenizer.batch_encode_plus(\n","    X_test.tolist(),\n","    truncation=True,\n","    padding=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","# Create the BERT model\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Define the sentiment classification model\n","class SentimentClassifier(nn.Module):\n","    def __init__(self, bert_model):\n","        super(SentimentClassifier, self).__init__()\n","        self.bert = bert_model\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc = nn.Linear(768, 1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        output = self.dropout(pooled_output)\n","        output = self.fc(output)\n","        return output\n","\n","# Initialize the sentiment classification model\n","classifier = SentimentClassifier(model)\n","\n","# Set the device for training\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","classifier.to(device)\n","\n","# Convert the data to tensors and move to the device\n","X_train_input_ids = X_train_encodings['input_ids'].to(device)\n","X_train_attention_mask = X_train_encodings['attention_mask'].to(device)\n","y_train = torch.tensor(y_train.values, dtype=torch.float).unsqueeze(1).to(device)\n","\n","X_test_input_ids = X_test_encodings['input_ids'].to(device)\n","X_test_attention_mask = X_test_encodings['attention_mask'].to(device)\n","y_test = torch.tensor(y_test.values, dtype=torch.float).unsqueeze(1).to(device)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-5)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    classifier.train()\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    outputs = classifier(X_train_input_ids, X_train_attention_mask)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward pass and optimization\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Evaluation on the testing set\n","    classifier.eval()\n","    with torch.no_grad():\n","        test_outputs = classifier(X_test_input_ids, X_test_attention_mask)\n","        test_predictions = torch.round(torch.sigmoid(test_outputs))\n","\n","    accuracy = accuracy_score(y_test.cpu().numpy(), test_predictions.cpu().numpy())\n","    # print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Accuracy: {\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDN7lhawVHdJ","outputId":"ccc217d9-210d-4d23-bd34-3283a26fa333"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch\n","import torch\n","from transformers import AdamW\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the dataset\n","df = pd.read_csv('/content/output_file.csv')\n","\n","# Preprocess the data\n","X = df['text'].values\n","aspects = df['aspect'].values\n","y = df['label'].values\n","\n","# Perform label encoding on the target variable\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(y)\n","\n","# Split the data into train and test sets\n","X_train, X_test, aspects_train, aspects_test, y_train, y_test = train_test_split(\n","    X, aspects, y, test_size=0.2, random_state=42\n",")\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","# Tokenize the input texts and aspects\n","tokenized_texts = []\n","for text, aspect in zip(X_train, aspects_train):\n","    tokenized_text = tokenizer.encode(text, aspect, add_special_tokens=True)\n","    tokenized_texts.append(tokenized_text)\n","\n","# Pad the tokenized sequences to have the same length\n","max_length = max(len(tokens) for tokens in tokenized_texts)\n","input_ids = []\n","attention_masks = []\n","for tokens in tokenized_texts:\n","    padding_length = max_length - len(tokens)\n","    input_id = tokens + [0] * padding_length\n","    attention_mask = [1] * len(tokens) + [0] * padding_length\n","    input_ids.append(input_id)\n","    attention_masks.append(attention_mask)\n","\n","# Convert the lists to tensors\n","input_ids = torch.tensor(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","labels = torch.tensor(y_train)\n","\n","# Create a DataLoader for training data\n","batch_size = 32\n","train_dataset = TensorDataset(input_ids, attention_masks, labels)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Load the pre-trained BERT model for sequence classification\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n","\n","# Set the model to training mode\n","model.train()\n","\n","# Set the optimizer and learning rate\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# Fine-tune the BERT model\n","epochs = 10\n","for epoch in range(epochs):\n","    total_loss = 0\n","    for batch in train_dataloader:\n","        batch_input_ids = batch[0].to(device)\n","        batch_attention_masks = batch[1].to(device)\n","        batch_labels = batch[2].to(device)\n","\n","        # Clear any previously calculated gradients\n","        model.zero_grad()\n","\n","        # Perform forward pass\n","        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        # Perform backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    average_loss = total_loss / len(train_dataloader)\n","    print('Epoch:', epoch+1, 'Average Loss:', average_loss)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Tokenize and pad the test data\n","tokenized_texts_test = []\n","for text, aspect in zip(X_test, aspects_test):\n","    tokenized_text = tokenizer.encode(text, aspect, add_special_tokens=True)\n","    tokenized_texts_test.append(tokenized_text)\n","\n","input_ids_test = []\n","attention_masks_test = []\n","for tokens in tokenized_texts_test:\n","    padding_length = max_length - len(tokens)\n","    input_id = tokens + [0] * padding_length\n","    attention_mask = [1] * len(tokens) + [0] * padding_length\n","    input_ids_test.append(input_id)\n","    attention_masks_test.append(attention_mask)\n","\n","input_ids_test = torch.tensor(input_ids_test)\n","attention_masks_test = torch.tensor(attention_masks_test)\n","labels_test = torch.tensor(y_test)\n","\n","# Create a DataLoader for test data\n","test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# Evaluate the model on the test data\n","predictions = []\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        batch_input_ids = batch[0].to(device)\n","        batch_attention_masks = batch[1].to(device)\n","        batch_labels = batch[2].to(device)\n","\n","        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n","        logits = outputs.logits\n","\n","        _, predicted_labels = torch.max(logits, dim=1)\n","        predictions.extend(predicted_labels.cpu().numpy())\n","\n","# Convert the predicted labels back to original classes\n","predicted_sentiments = label_encoder.inverse_transform(predictions)\n","\n","# Calculate accuracy, confusion matrix, and F1 score\n","from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n","\n","accuracy = accuracy_score(y_test, predicted_sentiments)\n","confusion_mat = confusion_matrix(y_test, predicted_sentiments)\n","f1 = f1_score(y_test, predicted_sentiments, average='weighted')\n","\n","print('Accuracy:', accuracy)\n","print('Confusion Matrix:', confusion_mat)\n","print('F1 Score:', f1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXAB1JxWWKFf","executionInfo":{"status":"ok","timestamp":1685125438459,"user_tz":-330,"elapsed":2580228,"user":{"displayName":"Anurag Jaiswal","userId":"15394690478600577889"}},"outputId":"588aeee3-5226-4c44-d760-700c650a1c1c"},"execution_count":3,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 Average Loss: 0.8677192402297053\n","Epoch: 2 Average Loss: 0.5502950975607181\n","Epoch: 3 Average Loss: 0.39067188187919816\n","Epoch: 4 Average Loss: 0.2919089503843209\n","Epoch: 5 Average Loss: 0.2302617396645505\n","Epoch: 6 Average Loss: 0.16177434223736153\n","Epoch: 7 Average Loss: 0.1137667353232873\n","Epoch: 8 Average Loss: 0.09858076661375575\n","Epoch: 9 Average Loss: 0.08694078948284531\n","Epoch: 10 Average Loss: 0.06850041749341221\n","Accuracy: 0.09503239740820735\n","Confusion Matrix: [[  0   0   0   0]\n"," [145  20  16   0]\n"," [ 26  64  24   0]\n"," [  9  13 146   0]]\n","F1 Score: 0.09564398589120063\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3ObMA7O6URCc"},"execution_count":null,"outputs":[]}]}