{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTPlF3R87cLuaBgACLW8q0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","!pip install transformers\n","from transformers import BertTokenizer, BertModel\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Set the path for the output file\n","path = '/content/output_file.csv'\n","\n","# Load the dataset from the CSV file\n","data = pd.read_csv(path)\n","\n","# Split the dataset into training and testing sets\n","X = data['text']\n","y = data['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize and encode the training data\n","X_train_encodings = tokenizer.batch_encode_plus(\n","    X_train.tolist(),\n","    truncation=True,\n","    padding=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","# Tokenize and encode the testing data\n","X_test_encodings = tokenizer.batch_encode_plus(\n","    X_test.tolist(),\n","    truncation=True,\n","    padding=True,\n","    max_length=256,\n","    return_tensors='pt'\n",")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAzQeRu_eX9-","executionInfo":{"status":"ok","timestamp":1684861335143,"user_tz":-330,"elapsed":26252,"user":{"displayName":"Anurag Jaiswal","userId":"15394690478600577889"}},"outputId":"54343786-0db4-4161-918f-e07e0d09ac23"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","source":["# Create the BERT model\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# Define the sentiment classification model\n","class SentimentClassifier(nn.Module):\n","    def __init__(self, bert_model):\n","        super(SentimentClassifier, self).__init__()\n","        self.bert = bert_model\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc = nn.Linear(768, 1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        output = self.dropout(pooled_output)\n","        output = self.fc(output)\n","        return output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxLEJnVojmbi","executionInfo":{"status":"ok","timestamp":1684861352363,"user_tz":-330,"elapsed":11268,"user":{"displayName":"Anurag Jaiswal","userId":"15394690478600577889"}},"outputId":"0aa5ec79-56ff-4697-fdd7-bfd5199fc7fa"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# Initialize the sentiment classification model\n","classifier = SentimentClassifier(model)\n","\n","# Set the device for training\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","classifier.to(device)\n","\n","# Convert the data to tensors and move to the device\n","X_train_input_ids = X_train_encodings['input_ids'].to(device)\n","X_train_attention_mask = X_train_encodings['attention_mask'].to(device)\n","y_train = torch.tensor(y_train.values, dtype=torch.float).unsqueeze(1).to(device)\n","\n","X_test_input_ids = X_test_encodings['input_ids'].to(device)\n","X_test_attention_mask = X_test_encodings['attention_mask'].to(device)\n","y_test = torch.tensor(y_test.values, dtype=torch.float).unsqueeze(1).to(device)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-5)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    classifier.train()\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    outputs = classifier(X_train_input_ids, X_train_attention_mask)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward pass and optimization\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Evaluation on the testing set\n","    classifier.eval()\n","    with torch.no_grad():\n","        test_outputs = classifier(X_test_input_ids, X_test_attention_mask)\n","        test_predictions = torch.round(torch.sigmoid(test_outputs))\n","\n","    accuracy = accuracy_score(y_test.cpu().numpy(), test_predictions.cpu().numpy())\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Accuracy: {accuracy}')\n","\n","# Final evaluation on the testing set\n","classifier.eval()\n","with torch.no_grad():\n","    test_outputs = classifier(X_test_input_ids, X_test_attention_mask)\n","    test_predictions = torch.round(torch.sigmoid(test_outputs))\n","\n","accuracy = accuracy_score(y_test.cpu().numpy(), test_predictions.cpu().numpy())\n","print(f'Final Accuracy: {accuracy}')\n"],"metadata":{"id":"9PlZbt9Iju5M"},"execution_count":null,"outputs":[]}]}